replacement = '.',
fixed = T))
top_family_meta <- family_tax_metadata %>%
gather(key = 'family', value = 'relative_abundance',
13:as.numeric(ncol(family_tax_metadata))) %>%
filter(family %in% top_10_families)
# Double check that it worked:
unique(top_family_meta$family)
plot.df <- top_family_meta
plot.df <- separate(plot.df, col = family, sep = 'f__', remove = T,
into = c('uplevel', 'family'))
plot.df$uplevel <- NULL
ggplot(plot.df, aes(color = family, x = timepoint, y = relative_abundance)) +
theme_bw() + geom_smooth(method='loess', span = 0.5) +
labs(x = 'months', y = 'relative abundance') +
theme(legend.position = 'right') + scale_x_continuous(breaks = c(0,12,24,36)) +
theme(panel.grid = element_blank())
# Test the difference between Bacteroidaceae abundance in
#  antibiotic exposed vs. non-exposed conditions
f__Bacteroidaceae.top_family_meta <- top_family_meta %>% filter(family == 'k__Bacteria.p__Bacteroidetes.c__Bacteroidia.o__Bacteroidales.f__Bacteroidaceae')
bacteroidaceae_result <- permuspliner(data = f__Bacteroidaceae.top_family_meta, xvar = 'timepoint',
yvar = 'relative_abundance', perms = 99,
category = 'antibiotics_y_n', cases = 'baby_id', retain_perm = T)
permuspliner.plot.permdistance(bacteroidaceae_result, xlabel = 'timepoint')
slide_result <- sliding_spliner(data = f__Bacteroidaceae.top_family_meta,
xvar = 'timepoint', yvar = 'relative_abundance',
category = 'antibiotics_y_n', cases = 'baby_id',
test_density = 10, cut_low = 7,
set_spar = 0.5)
library(dplyr)
library(tibble)
library(ggplot2)
library(reshape2)
library(tidyr)
library(splinectomeR)
library(vegan)
# source link: https://rrshieldscutler.github.io/splinectomeR/yassour_antibiotics_web.html
# Create the binary antibiotic exposure annotation: data existing within splinectomeR package
abx <- antibiotics_metadata
abx_pos <- unique(abx$Subject)
abx_df <- data.frame(abx_pos, 'Y')
abx_neg <- setdiff(timeseries$sample_id, abx_pos)
abx_df2 <- setNames(data.frame(abx_neg, 'N'), c('abx_pos', 'X.Y.'))
abx_df <- rbind(abx_df, abx_df2)
colnames(abx_df) <- c('baby_id', 'antibiotics_y_n')
dim(abx_df)
head(abx_df, n=4)
# Now, merge OTUs with each piece of the metadata
otus <- filtered_otu_table
otus <- data.frame(t(otus))
otus <- tibble::rownames_to_column(otus, var = 'sampleID')
otus_split <- otus %>% separate(sampleID, c('baby_id', 'timepoint'), sep = '_')
row.names(otus_split) <- otus$sampleID
otus_split <- rownames_to_column(otus_split, var = '#SampleID')
metadata <- general_metadata
colnames(metadata)[1] <- 'baby_id'
metadata <- merge(metadata, abx_df, by = 'baby_id')
# Merge the metadata with the OTU table to create a master metadata
# file including the taxonomy abundances
full_data <- merge(metadata, otus_split, by = 'baby_id')
x <- '#SampleID'
# Move the "#SampleID" column to the front:
full_data <- full_data[c(x, setdiff(names(full_data), x))]
# focus on family level only
otus <- filtered_otu_table
otus <- tibble::rownames_to_column(otus, var = 'OTU_ID')
otus_family <- otus %>% filter(!grepl('g__', otus$OTU_ID) & grepl('f__', otus$OTU_ID))
otus_family_sum <- otus_family
otus_family_sum$f__abun <- rowSums(otus_family[, 2:ncol(otus_family)])
otus_family_sum <- otus_family_sum[c('f__abun',
setdiff(names(otus_family_sum), 'f__abun'))]
otus_family_sum <- otus_family_sum[order(-otus_family_sum$f__abun), ]
# write_delim(otus_family_sum, 'family_level_OTUs_with_sums.txt', delim = '\t')
# The top 10 Families in this dataset
otus_family_sum[1:10, 2]
colSums(otus_family_sum[, 3:10])
# Define a function for all the tedious flipping, splitting, and merging
flip_split_merge <- function(otus_in, metadata) {
row.names(otus_in) <- otus_in$OTU_ID
otus_in$OTU_ID <- NULL
otus_in <- data.frame(t(otus_in))
otus_in <- tibble::rownames_to_column(otus_in, var = 'sampleID')
otus_split <- otus_in %>% separate(sampleID, c('baby_id', 'timepoint'), sep = '_')
otus_split$timepoint <- as.numeric(otus_split$timepoint)  # Trouble recognizing numbers
row.names(otus_split) <- otus_in$sampleID
otus_split <- rownames_to_column(otus_split, var = 'SampleID')
otus_meta <- merge(metadata, otus_split, by = 'baby_id')
otus_meta <- otus_meta[c('SampleID', setdiff(names(otus_meta), 'SampleID'))]
return(otus_meta)
}
otus_genus <- otus %>% filter(!grepl('s__', otus$OTU_ID) & grepl('g__', otus$OTU_ID))
alphadiv <- as.data.frame(diversity(t(otus_genus[,2:ncol(otus_genus)]), 'shannon'))
alphadiv <- tibble::rownames_to_column(alphadiv, var = 'sampleID')
colnames(alphadiv)[2] <- 'shannon'
alphadiv <- alphadiv %>% separate(sampleID, c('baby_id', 'timepoint'), sep = '_')
alphadiv$timepoint <- as.numeric(alphadiv$timepoint)
alphadiv$shannon <- as.numeric(alphadiv$shannon)
alphadiv_meta <- merge(metadata, alphadiv, by = 'baby_id')
alpha_permresult_abx <- permuspliner(data = alphadiv_meta, xvar = 'timepoint',
yvar = 'shannon', category = 'antibiotics_y_n',
cases = 'baby_id', perms = 99, retain_perm = T,
quiet = T)
alpha_permresult_mode <- permuspliner(data = alphadiv_meta, xvar = 'timepoint',
yvar = 'shannon', category = 'birth_mode',
cases = 'baby_id', perms = 99, retain_perm = T,
quiet = T)
p_abx <- permuspliner.plot.permsplines(data = alpha_permresult_abx,
xvar = 'timepoint', yvar = 'shannon')
p_mode <- permuspliner.plot.permsplines(data = alpha_permresult_mode,
xvar = 'timepoint', yvar = 'shannon')
p_abx
head(alphadiv_meta)
alpha_permresult_abx <- permuspliner(data = alphadiv_meta, xvar = 'timepoint',
yvar = 'shannon', category = 'antibiotics_y_n',
cases = 'baby_id', perms = 99, retain_perm = T,
quiet = T)
alpha_permresult_abx <- permuspliner(data = alphadiv_meta, xvar = 'timepoint',
yvar = 'shannon', category = 'antibiotics_y_n',
cases = 'baby_id', perms = 99, retain_perm = T,
quiet = T)
data(varespec)
data(varechem)
library(vegan)
data(varespec)
data(varechem)
cca(varespec ~ Ca, varechem)
cca(varespec ~ Ca + Condition(pH), varechem)
rda(varespec ~ Ca + Condition(pH), varechem)
0.1282  + 0.1212 + 0.7506
data(dune, dune.env)
mod <- rda(dune, scale = TRUE)
plot(mod)
colvec <- c("red2", "green4", "mediumblue")
head(dune)
with(dune.env, points(mod, display = "sites", col = colvec[Use], pch=21))
with(dune.env, points(mod, display = "sites", col = colvec[Use],
scaling = scl, pch = 21, bg = colvec[Use]))
with(dune.env, points(mod, display = "sites", col = colvec[Use],
, pch = 21, bg = colvec[Use]))
with(dune.env, legend("topright", legend = levels(Use), bty = "n",
col = colvec, pch = 21, pt.bg = colvec))
with(dune.env, legend("topright", legend = levels(Use), bty = "n",
col = colvec, pch = 21, pt.bg = colvec))
require("vegan")
## load the Dune data
data(dune, dune.env)
## PCA of the Dune data
mod <- rda(dune, scale = TRUE)
colvec <- c("red2", "green4", "mediumblue")
scl <- 3
with(dune.env, legend("topright", legend = levels(Use), bty = "n",
col = colvec, pch = 21, pt.bg = colvec))
with(dune.env, points(mod, display = "sites", col = colvec[Use],
scaling = scl, pch = 21, bg = colvec[Use]))
plot(mod, type = "n", scaling = scl)
with(dune.env, legend("topright", legend = levels(Use), bty = "n",
col = colvec, pch = 21, pt.bg = colvec))
with(dune.env, points(mod, display = "sites", col = colvec[Use],
scaling = scl, pch = 21, bg = colvec[Use]))
plot(mod)
plot(mod, choices=c(1,3))
library(ggplot2)
library(reshape2)
#library(splinectomeR)
colpal <- c("#E69F00", "#56B4E9", "#009E73", "#CC79A7")
df_orig <- data.frame(matrix(ncol=3, nrow = 120))
colnames(df_orig) <- c('patient','time','response')
ids <- c(1,2,3,4,5,6,7,8,9,10)
timeseries <- c(0,2,4,6,8,10,12,14,16,18,20,22)
patient <- unlist(lapply(X = ids, FUN = function(xx) rep(xx, times=12)))
set.seed(7)
obs <- c(rnorm(n=10, mean = 100, sd = 2),  # Make a distribution at each timepoint
rnorm(n=10, mean = 100, sd = 4),
rnorm(n=10, mean = 100, sd = 6),
rnorm(n=10, mean = 100, sd = 8),
rnorm(n=10, mean = 100, sd = 6),
rnorm(n=10, mean = 100, sd = 4),
rnorm(n=10, mean = 100, sd = 6),
rnorm(n=10, mean = 100, sd = 8),
rnorm(n=10, mean = 100, sd = 6),
rnorm(n=10, mean = 100, sd = 4),
rnorm(n=10, mean = 100, sd = 3),
rnorm(n=10, mean = 100, sd = 2))
df_orig$patient <- patient
df_orig$time <-rep(timeseries, times = 10)
df_orig$response <- obs
df <- df_orig
head(df)
summary(df[df$patient == 1, ])
summary(df[df$patient == 2, ])
summary(df[df$patient == 3, ])
length(unique(df$time))
ggplot(df) + geom_smooth(aes(x=time, y=response, color=Version), se = F, method = 'loess') +
geom_point(aes(x=time, y=response, color=Version), alpha=0.5, size=1.2) +
theme_classic() + theme(axis.text = element_text(color='black')) +
scale_color_manual(values=colpal)
library(readxl)
toy <- read_excel("Desktop/toy.xlsx")
View(toy)
head(toy)
colnames(toy) = toy$boostrap
head(toy)
View(toy)
library(readxl)
toy <- read_excel("Desktop/toy.xlsx")
View(toy)
rownames(toy) = toy$boostrap
head(toy)
A = toy[, -1]
head(A)
getStability <- function(X,alpha=0.05) {
## the input X is a binary matrix of size M*d where:
## M is the number of bootstrap replicates
## d is the total number of features
## alpha is the level of significance (e.g. if alpha=0.05, we will get 95% confidence intervals)
## it's an optional argument and is set to 5% by default
### first we compute the stability
M<-nrow(X)
d<-ncol(X)
hatPF<-colMeans(X)
kbar<-sum(hatPF)
v_rand=(kbar/d)*(1-kbar/d)
stability<-1-(M/(M-1))*mean(hatPF*(1-hatPF))/v_rand ## this is the stability estimate
return(stability)}
getStability(A)
library(readxl)
toy2 <- read_excel("Desktop/toy2.xlsx")
View(toy2)
B = toy2[, -1]
head(B)
getStability(B)
idx.start = 1
idx.stop = 100
idx.stop - idx.start + 1
cv.fold=10
n = 100
n.cv.tr = floor(n*(cv.fold - 1)/cv.fold)
n.cv.tr
n / cv.fold
floor(n/cv.fold)
library(caret)
library(mlbench)
data(Sonar)
head(Sonar)
flds = createFolds(Sonar$Class, k=10)
flds
flds[[1]]
flds = createFolds(Sonar$Class, k=10)
flds[[1]]
data = Sonar
testData <- data[testIndexes, ]
i = 1
testIndexes <- flds[[i]]
testData <- data[testIndexes, ]
trainData <- data[-testIndexes, ]
dim(testData)
dim(trainData)
3233 + 2173
5406-4273
17 * 4
16 * 4
900 * 0.8
library(readr)
Wright_Depner_analysis <- read_csv("Study/thesis/Bayesian/sfpca/Antonio_Sleep/Wright_Depner_analysis.tsv")
View(Wright_Depner_analysis)
Wright_Depner_analysis <- read.delim("~/Study/thesis/Bayesian/sfpca/Antonio_Sleep/Wright_Depner_analysis.tsv", comment.char="#")
View(Wright_Depner_analysis)
dat = Wright_Depner_analysis
Wright_Depner_analysis <- read.delim("~/Study/thesis/Bayesian/sfpca/Antonio_Sleep/Wright_Depner_analysis.tsv")
View(Wright_Depner_analysis)
dat = Wright_Depner_analysis
table(dat$qiita_study_id)
library(parallel)
library(rstan)
library(loo) # for waic()
library(Matrix) # for bdiag(): construct a block diagnoal matrix
options(mc.cores = parallel::detectCores())
############################################################
############ Data Preparation ##############################
############################################################
dat = read.csv("../data/map_alpha_closedref.txt", header=TRUE, sep='\t', na.strings='N/A')
dat = dat[, c('X.SampleID', 'subject', 'timepoint', 'shannon_alpha', 'sample_site', 'sex')]
dat$timepoint = sub("^\\S+\\s+", '', dat$timepoint)
dat = dat[complete.cases(dat$shannon_alpha), ] # complete response (PD and shannon share same missing index)
dat = dat[!(dat$subject %in% c('Blank', 'swab blank', 'Swab blank', 'Beauty product', 'Skin')), ]
dim(dat) # 1560 * 6
## new unique ID
dat$ID_unique = paste(dat$subject, dat$sample_site, sep='')
## take average of duplicated alpha diversity
dat$shannon_avg = rep(0, dim(dat)[1])
pid_list = unique(dat$ID_unique)
sub = NULL
time_sampling = unique(dat$timepoint)
for (i in pid_list){
for (t in time_sampling){
idx = which(dat$ID_unique %in% i & dat$timepoint %in% t)
dat$shannon_avg[idx] = rep(mean(dat$shannon_alpha[idx]), length(idx))
sub = rbind(sub, dat[idx[1], ])
}
}
sub = sub[complete.cases(sub$shannon_avg), ] # not al
setwd("~/Study/thesis/Bayesian/sfpca/simulation_microbiome/code_2D/sim_Babies_1D_2G/N100_C20")
load('estimates_N100_Orth_G2.RData')
ALPHA_array = alpha_new
MU_array = theta_mu_new
THETA_array = Theta_new
phi_t_cont = results_basis$orth_spline_basis_cont
time_cont = results_basis$time_cont
N = prepared_data$num_subjects
MU_true = dat$params[[7]][(1+Q1):(Q+Q1)]
FPC_true = dat$params[[8]][, (1+K1):(K+K1)]
# re-run the following to make sure
time = Y = list()
ids=rep(1:N,each=1)
for (i in 1:N){
time[[i]] = dat$TIME_SPARSE[[1]][[i]][[J]]
Y[[i]] = dat$Y_SPARSE[[1]][[i]][[J]]
}
time_sparse = time # coming from wide format: sampling time point for each subject
Y_sparse = Y # coming from wide format: response for each subject
time_unique = dat$time
nloop=dim(ALPHA_array)[3]
first=1
last=nloop
library(parallel)
library(rstan)
library(loo)
library(Matrix)
options(mc.cores = parallel::detectCores())
source('../../../../sfpca.R')
#### test on simulated code
load("sim_N100_Orth.RData", dat <- new.env())
load('estimates_N100_Orth_G2.RData')
ALPHA_array = alpha_new
MU_array = theta_mu_new
THETA_array = Theta_new
phi_t_cont = results_basis$orth_spline_basis_cont
time_cont = results_basis$time_cont
N = prepared_data$num_subjects
MU_true = dat$params[[7]][(1+Q1):(Q+Q1)]
FPC_true = dat$params[[8]][, (1+K1):(K+K1)]
# re-run the following to make sure
time = Y = list()
ids=rep(1:N,each=1)
for (i in 1:N){
time[[i]] = dat$TIME_SPARSE[[1]][[i]][[J]]
Y[[i]] = dat$Y_SPARSE[[1]][[i]][[J]]
}
time_sparse = time # coming from wide format: sampling time point for each subject
Y_sparse = Y # coming from wide format: response for each subject
time_unique = dat$time
nloop=dim(ALPHA_array)[3]
first=1
last=nloop
library(parallel)
library(rstan)
library(loo)
library(Matrix)
options(mc.cores = parallel::detectCores())
source('../../../../sfpca.R')
#### test on simulated code
load("sim_N100_Orth.RData", dat <- new.env())
#ls.str(dat)
## true values of parameters
J = 2
N = length(dat$Y_SPARSE[[1]])
nknots = dat$params[[1]][[J]]
Q = nknots + 4 # number of basis
K = dat$params[[2]][[J]] # number of PC
sigma_eps = sqrt(dat$params[[5]][[J]]) # SIGMA_OMEGA_true is error variance
Q1 = dat$params[[1]][[1]] + 4
K1 = dat$params[[2]][[1]]
load('estimates_N100_Orth_G2.RData')
ALPHA_array = alpha_new
MU_array = theta_mu_new
THETA_array = Theta_new
phi_t_cont = results_basis$orth_spline_basis_cont
time_cont = results_basis$time_cont
N = prepared_data$num_subjects
MU_true = dat$params[[7]][(1+Q1):(Q+Q1)]
FPC_true = dat$params[[8]][, (1+K1):(K+K1)]
# re-run the following to make sure
time = Y = list()
ids=rep(1:N,each=1)
for (i in 1:N){
time[[i]] = dat$TIME_SPARSE[[1]][[i]][[J]]
Y[[i]] = dat$Y_SPARSE[[1]][[i]][[J]]
}
time_sparse = time # coming from wide format: sampling time point for each subject
Y_sparse = Y # coming from wide format: response for each subject
time_unique = dat$time
nloop=dim(ALPHA_array)[3]
first=1
last=nloop
pdf('Mean_trueVsestimated.pdf', width = 4, height = 4)
plot(time_cont,Mu1_true,type="l",ylim=c(-10,10), xlim=c(0,1),lwd=2,col=4,
xlab='time', ylab='Response', font.lab=2, cex.lab=1.2)
for(i in 1:N){
lines(time_sparse[[i]],Y_sparse[[i]],type="l",lwd=.25)
}
lines(time_cont,Mu1,type="l",col=2,lwd=2)
#title('True vs. Estimated Mean')
legend('topright', c('True', 'Estimated'), col=c(4, 2), lty=c(1,1), bty='n')
dev.off()
#plot PC functions
THETA_mean = THETA_array[,,first]
for(iter in 2:nloop){
THETA_mean = THETA_mean + THETA_array[,,iter]
}
THETA_mean=cbind(THETA_mean/(last-first+1))
FPC1_mean=t(phi_t_cont)%*%THETA_mean # for 1st block
pdf('FPCs_trueVsestimated.pdf', width = 4, height = 4)
plot(time_unique,FPC_true[,1],type="l",lwd=2,ylim=c(min(FPC_true),max(FPC_true)),
xlab='time', ylab='PC curve', font.lab=2, cex.lab=1.2)
lines(time_unique,FPC_true[,2],type="l",lwd=1)
lines(time_cont,FPC1_mean[,1],type="l",lwd=2,col=2)
lines(time_cont,FPC1_mean[,2],type="l",lwd=1,col=2)
#title(main="True vs. Estimated PFCs")
legend('topright', c('True', 'Estimated'), col=c(1, 2), lty=c(1,1), bty='n')
dev.off()
# pdf('plot_stan.pdf')
# par(mfcol=c(2,2))
#plot mean functions
MU_mean=MU_array[, 1]
for(iter in 2:nloop){
MU_mean = MU_mean + MU_array[, iter]
}
MU_mean=cbind(MU_mean/(last-first+1))
Mu_true_functions=t(bdiag(cbind(phi_t_cont)))%*%MU_true
Mu_functions=t(bdiag(cbind(phi_t_cont)))%*%MU_mean
Mu1_true=Mu_true_functions[1:length(time_cont)] # referring to 1st block
Mu1=Mu_functions[1:length(time_cont)]
pdf('Mean_trueVsestimated.pdf', width = 4, height = 4)
plot(time_cont,Mu1_true,type="l",ylim=c(-10,10), xlim=c(0,1),lwd=2,col=4,
xlab='time', ylab='Response', font.lab=2, cex.lab=1.2)
for(i in 1:N){
lines(time_sparse[[i]],Y_sparse[[i]],type="l",lwd=.25)
}
lines(time_cont,Mu1,type="l",col=2,lwd=2)
#title('True vs. Estimated Mean')
legend('topright', c('True', 'Estimated'), col=c(4, 2), lty=c(1,1), bty='n')
dev.off()
#plot PC functions
THETA_mean = THETA_array[,,first]
for(iter in 2:nloop){
THETA_mean = THETA_mean + THETA_array[,,iter]
}
THETA_mean=cbind(THETA_mean/(last-first+1))
FPC1_mean=t(phi_t_cont)%*%THETA_mean # for 1st block
pdf('FPCs_trueVsestimated.pdf', width = 4, height = 4)
plot(time_unique,FPC_true[,1],type="l",lwd=2,ylim=c(min(FPC_true),max(FPC_true)),
xlab='time', ylab='PC curve', font.lab=2, cex.lab=1.2)
lines(time_unique,FPC_true[,2],type="l",lwd=1)
lines(time_cont,FPC1_mean[,1],type="l",lwd=2,col=2)
lines(time_cont,FPC1_mean[,2],type="l",lwd=1,col=2)
#title(main="True vs. Estimated PFCs")
legend('topright', c('True', 'Estimated'), col=c(1, 2), lty=c(1,1), bty='n')
dev.off()
load('estimates_N100_Orth_G2.RData')
K = npcs = 2  # revised manually; think about saving its value, also Q, earlier
ALPHA_array = alpha_new
MU_array = theta_mu_new
THETA_array = Theta_new
phi_t_cont = results_basis$orth_spline_basis_cont
phi_t = results_basis$orth_spline_basis_sparse
time_cont = results_basis$time_cont
nloop=dim(ALPHA_array)[3]
first=1
last=nloop
N = prepared_data$num_subjects
MU_mean = MU_array[, first] #mean function across sampling sessions
ALPHA_mean = ALPHA_array[,,first] # mean factor scores
THETA_mean = THETA_array[,,first] # mean factor loading
for(iter in 2:nloop){
MU_mean = MU_mean + MU_array[, iter]
ALPHA_mean = ALPHA_mean + ALPHA_array[,,iter]
THETA_mean = THETA_mean + THETA_array[,,iter]
}
MU_mean=cbind(MU_mean/(last-first+1))
ALPHA_mean=cbind(ALPHA_mean/(last-first+1))
THETA_mean=cbind(THETA_mean/(last-first+1))
Mu_functions = t(bdiag(cbind(phi_t_cont)))%*%MU_mean
FPC_mean=t(phi_t_cont)%*%THETA_mean
df = prepared_data$data
Y_sparse = list()
time_sparse = list()
scores = data.frame(t(ALPHA_mean))
names(scores)=c("fpc1","fpc2")
df$fpc1=0 # principle component scores
df$fpc2=0
i = 0
for (pid in unique(df$ID)){
i = i + 1
Y_sparse[[i]] = df$response[df$ID == pid]
time_sparse[[i]] = df$time[df$ID == pid]
df$fpc1[df$ID == pid] = scores[i, 1]
df$fpc2[df$ID == pid] = scores[i, 2]
}
Fits_sparse=list()
for(i in 1:N){
Fits_sparse[[i]] = t(phi_t[[i]]) %*% MU_mean + t(phi_t[[i]]) %*% THETA_mean %*% ALPHA_mean[, i]
}
pdf('fpcScores_scatterplots.pdf', width=4, height=4)
colors = rep('black', length(df$group))
colors[df$group == 'G1'] = 'pink'
colors[df$group == 'G2'] = 'green'
plot(df$fpc1, df$fpc2, pch=20, cex=1,
xlab='PC1 score', ylab='PC2 score', col=colors,
font.lab=2, cex.lab=1.2)
legend(x='topright', legend=c("G1", 'G2'),
col=c('pink', 'green'), pch=c(20,20), cex=0.8)
dev.off()
